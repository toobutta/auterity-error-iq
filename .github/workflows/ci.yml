name: CI Pipeline with Quality Gates

on:
  push:
    branches:
      - main
    paths-ignore:
      - '**/*.md'
  pull_request:
    branches:
      - main
    paths-ignore:
      - '**/*.md'

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  QUALITY_GATE_BLOCKING: true
  SECURITY_SEVERITY_THRESHOLD: HIGH
  PERFORMANCE_DEGRADATION_THRESHOLD: 10
  TEST_COVERAGE_MINIMUM: 80

jobs:
  setup:
    runs-on: ubuntu-latest
    outputs:
      backend-cache-key: ${{ steps.backend-cache-key.outputs.key }}
      frontend-cache-key: ${{ steps.frontend-cache-key.outputs.key }}
    steps:
      - uses: actions/checkout@v4

      - name: Generate backend cache key
        id: backend-cache-key
        run: echo "key=backend-${{ hashFiles('backend/requirements.txt') }}-$(date +'%Y-%m')" >> $GITHUB_OUTPUT

      - name: Generate frontend cache key
        id: frontend-cache-key
        run: echo "key=frontend-${{ hashFiles('frontend/package-lock.json') }}-$(date +'%Y-%m')" >> $GITHUB_OUTPUT

  backend-test:
    needs: setup
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'
        cache: 'pip'
        cache-dependency-path: backend/requirements.txt

    - name: Cache Python dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ needs.setup.outputs.backend-cache-key }}
        restore-keys: |
          backend-${{ hashFiles('backend/requirements.txt') }}
          backend-

    - name: Install dependencies
      run: |
        cd backend
        pip install -r requirements.txt

    - name: Run tests
      run: |
        cd backend
        python -m pytest tests/ -v --junitxml=test-results.xml

    - name: Upload test results
      uses: actions/upload-artifact@v3
      with:
        name: backend-test-results
        path: backend/test-results.xml
        retention-days: 5

  frontend-test:
    needs: setup
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Cache node modules
      uses: actions/cache@v3
      with:
        path: frontend/node_modules
        key: ${{ needs.setup.outputs.frontend-cache-key }}
        restore-keys: |
          frontend-${{ hashFiles('frontend/package-lock.json') }}
          frontend-

    - name: Install dependencies
      run: |
        cd frontend
        npm ci

    - name: Run tests
      run: |
        cd frontend
        npm test -- --coverage --watchAll=false --maxWorkers=2

    - name: Build
      run: |
        cd frontend
        npm run build

    - name: Cache build output
      uses: actions/cache@v3
      with:
        path: frontend/build
        key: frontend-build-${{ github.sha }}

    - name: Upload build artifacts
      uses: actions/upload-artifact@v3
      with:
        name: frontend-build
        path: frontend/build
        retention-days: 5

  security-scan:
    runs-on: ubuntu-latest
    needs: [backend-test, frontend-test]
    outputs:
      security-status: ${{ steps.security-check.outputs.status }}
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install security tools
      run: |
        pip install pip-audit safety bandit
        npm install -g npm-audit-resolver snyk @cyclonedx/cyclonedx-cli

    - name: Run frontend security audit
      id: frontend-security
      run: |
        cd frontend
        echo "Running npm audit..."
        npm audit --audit-level ${{ env.SECURITY_SEVERITY_THRESHOLD == 'HIGH' && 'moderate' || 'info' }} --json > audit-results.json || true
        npm audit --audit-level ${{ env.SECURITY_SEVERITY_THRESHOLD == 'HIGH' && 'moderate' || 'info' }} || { echo "FRONTEND_SECURITY_FAILED=true" >> $GITHUB_OUTPUT; exit 0; }

    - name: Run backend security audit
      id: backend-security
      run: |
        cd backend
        echo "Running pip-audit..."
        pip-audit -r requirements.txt --format json > pip-audit-results.json || true
        pip-audit -r requirements.txt || { echo "BACKEND_SECURITY_FAILED=true" >> $GITHUB_OUTPUT; exit 0; }

        echo "Running Bandit security linter..."
        bandit -r . -f json -o bandit-results.json || true
        bandit -r . -f txt || { echo "BACKEND_SECURITY_FAILED=true" >> $GITHUB_OUTPUT; exit 0; }

    - name: Run Snyk security scan
      id: snyk-scan
      run: |
        echo "Running Snyk security scan..."
        cd frontend && snyk test --json > snyk-frontend-results.json || true
        cd ../backend && snyk test --json > snyk-backend-results.json || true

        # Check if Snyk found critical vulnerabilities
        if [[ -f "frontend/snyk-frontend-results.json" ]]; then
          critical_count=$(cat frontend/snyk-frontend-results.json | jq '.vulnerabilities | length')
          if [[ $critical_count -gt 0 ]]; then
            echo "SNYK_SECURITY_FAILED=true" >> $GITHUB_OUTPUT
          fi
        fi

    - name: Generate security report
      id: security-report
      run: |
        echo "## Security Scan Report" > security-report.md
        echo "- **Timestamp:** $(date)" >> security-report.md
        echo "- **Commit:** ${{ github.sha }}" >> security-report.md
        echo "- **Branch:** ${{ github.ref_name }}" >> security-report.md
        echo "" >> security-report.md

        if [[ -f "frontend/audit-results.json" ]]; then
          vulnerabilities=$(cat frontend/audit-results.json | jq '.vulnerabilities | length')
          echo "- **Frontend Vulnerabilities:** $vulnerabilities" >> security-report.md
        fi

        if [[ -f "backend/pip-audit-results.json" ]]; then
          pip_vulns=$(cat backend/pip-audit-results.json | jq '.vulnerabilities | length')
          echo "- **Backend Python Vulnerabilities:** $pip_vulns" >> security-report.md
        fi

        if [[ -f "backend/bandit-results.json" ]]; then
          bandit_issues=$(cat backend/bandit-results.json | jq '.results | length')
          echo "- **Backend Security Issues:** $bandit_issues" >> security-report.md
        fi

        echo "" >> security-report.md
        echo "### Security Gate Status" >> security-report.md
        if [[ "${{ steps.frontend-security.outputs.FRONTEND_SECURITY_FAILED }}" == "true" || "${{ steps.backend-security.outputs.BACKEND_SECURITY_FAILED }}" == "true" || "${{ steps.snyk-scan.outputs.SNYK_SECURITY_FAILED }}" == "true" ]]; then
          echo "âŒ **FAILED** - Security vulnerabilities detected above threshold" >> security-report.md
          echo "SECURITY_GATE_FAILED=true" >> $GITHUB_OUTPUT
        else
          echo "âœ… **PASSED** - No critical security issues found" >> security-report.md
          echo "SECURITY_GATE_FAILED=false" >> $GITHUB_OUTPUT
        fi

    - name: Upload security artifacts
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: |
          frontend/audit-results.json
          backend/pip-audit-results.json
          backend/bandit-results.json
          frontend/snyk-frontend-results.json
          backend/snyk-backend-results.json
          security-report.md
        retention-days: 7

    - name: Security gate check
      id: security-check
      run: |
        if [[ "${{ steps.security-report.outputs.SECURITY_GATE_FAILED }}" == "true" ]]; then
          echo "status=failed" >> $GITHUB_OUTPUT
          if [[ "${{ env.QUALITY_GATE_BLOCKING }}" == "true" ]]; then
            echo "ðŸ”’ Security quality gate failed - blocking pipeline"
            exit 1
          else
            echo "âš ï¸  Security quality gate failed - continuing (blocking disabled)"
          fi
        else
          echo "status=passed" >> $GITHUB_OUTPUT
          echo "âœ… Security quality gate passed"
        fi

  performance-test:
    runs-on: ubuntu-latest
    needs: [setup, backend-test, frontend-test]
    outputs:
      performance-status: ${{ steps.performance-check.outputs.status }}
      frontend-bundle-size: ${{ steps.bundle-analysis.outputs.bundle-size }}
      api-response-time: ${{ steps.api-performance.outputs.avg-response-time }}
    steps:
    - uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Restore frontend build cache
      uses: actions/cache@v3
      with:
        path: frontend/build
        key: frontend-build-${{ github.sha }}

    - name: Install frontend dependencies
      run: |
        cd frontend
        npm ci

    - name: Bundle size analysis
      id: bundle-analysis
      run: |
        cd frontend
        npm install -g bundlesize
        npm run build

        # Analyze bundle size
        echo "Analyzing bundle sizes..."
        bundle_size=$(du -sb build | cut -f1)
        echo "bundle-size=$bundle_size" >> $GITHUB_OUTPUT

        # Check against size limits (50MB limit)
        max_size=$((50 * 1024 * 1024))
        if [[ $bundle_size -gt $max_size ]]; then
          echo "BUNDLE_SIZE_EXCEEDED=true" >> $GITHUB_OUTPUT
          echo "âŒ Bundle size ($bundle_size bytes) exceeds limit ($max_size bytes)"
        else
          echo "BUNDLE_SIZE_EXCEEDED=false" >> $GITHUB_OUTPUT
          echo "âœ… Bundle size ($bundle_size bytes) within limit"
        fi

    - name: Set up Python for performance testing
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Install performance testing tools
      run: |
        pip install locust k6
        cd frontend && npm install -g lighthouse

    - name: Start application for testing
      run: |
        # Start backend in background
        cd backend
        pip install -r requirements.txt
        nohup python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &

        # Wait for backend to be ready
        timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'

        # Start frontend in background
        cd ../frontend
        nohup npm start &
        timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done'

    - name: API Performance Testing
      id: api-performance
      run: |
        echo "Running API performance tests..."

        # Create Locust test file
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import json

        class APIUser(HttpUser):
            wait_time = between(1, 3)
            host = "http://localhost:8000"

            @task(3)
            def get_workflows(self):
                self.client.get("/api/workflows/")

            @task(2)
            def health_check(self):
                self.client.get("/health")

            @task(1)
            def get_user_profile(self):
                headers = {"Authorization": "Bearer test-token"}
                self.client.get("/api/auth/me", headers=headers)
        EOF

        # Run performance test
        locust -f locustfile.py --host=http://localhost:8000 --users=10 --spawn-rate=2 --run-time=30s --csv=locust_results --headless

        # Analyze results
        if [[ -f "locust_results_stats.csv" ]]; then
          avg_response_time=$(tail -1 locust_results_stats.csv | cut -d',' -f6)
          echo "avg-response-time=$avg_response_time" >> $GITHUB_OUTPUT

          # Check performance threshold (2 seconds)
          if (( $(echo "$avg_response_time > 2000" | bc -l) )); then
            echo "API_PERFORMANCE_FAILED=true" >> $GITHUB_OUTPUT
            echo "âŒ Average API response time ($avg_response_time ms) exceeds threshold (2000ms)"
          else
            echo "API_PERFORMANCE_FAILED=false" >> $GITHUB_OUTPUT
            echo "âœ… Average API response time ($avg_response_time ms) within threshold"
          fi
        else
          echo "avg-response-time=0" >> $GITHUB_OUTPUT
          echo "API_PERFORMANCE_FAILED=true" >> $GITHUB_OUTPUT
        fi

    - name: Frontend Performance Testing
      id: frontend-performance
      run: |
        echo "Running frontend performance tests..."
        cd frontend

        # Run Lighthouse performance audit
        npx lighthouse http://localhost:3000 --output=json --output-path=lighthouse-results.json --quiet

        if [[ -f "lighthouse-results.json" ]]; then
          performance_score=$(cat lighthouse-results.json | jq '.categories.performance.score * 100')
          echo "lighthouse-score=$performance_score" >> $GITHUB_OUTPUT

          # Check performance score threshold (70)
          if (( $(echo "$performance_score < 70" | bc -l) )); then
            echo "FRONTEND_PERFORMANCE_FAILED=true" >> $GITHUB_OUTPUT
            echo "âŒ Lighthouse performance score ($performance_score) below threshold (70)"
          else
            echo "FRONTEND_PERFORMANCE_FAILED=false" >> $GITHUB_OUTPUT
            echo "âœ… Lighthouse performance score ($performance_score) meets threshold"
          fi
        else
          echo "lighthouse-score=0" >> $GITHUB_OUTPUT
          echo "FRONTEND_PERFORMANCE_FAILED=true" >> $GITHUB_OUTPUT
        fi

    - name: Generate performance report
      id: performance-report
      run: |
        echo "## Performance Test Report" > performance-report.md
        echo "- **Timestamp:** $(date)" >> performance-report.md
        echo "- **Commit:** ${{ github.sha }}" >> performance-report.md
        echo "- **Branch:** ${{ github.ref_name }}" >> performance-report.md
        echo "" >> performance-report.md

        echo "### Performance Metrics" >> performance-report.md
        echo "- **Bundle Size:** ${{ steps.bundle-analysis.outputs.bundle-size }} bytes" >> performance-report.md
        echo "- **API Response Time:** ${{ steps.api-performance.outputs.avg-response-time }} ms" >> performance-report.md
        echo "- **Lighthouse Score:** ${{ steps.frontend-performance.outputs.lighthouse-score }}" >> performance-report.md
        echo "" >> performance-report.md

        echo "### Performance Gate Status" >> performance-report.md
        if [[ "${{ steps.bundle-analysis.outputs.BUNDLE_SIZE_EXCEEDED }}" == "true" || "${{ steps.api-performance.outputs.API_PERFORMANCE_FAILED }}" == "true" || "${{ steps.frontend-performance.outputs.FRONTEND_PERFORMANCE_FAILED }}" == "true" ]]; then
          echo "âŒ **FAILED** - Performance metrics below threshold" >> performance-report.md
          echo "PERFORMANCE_GATE_FAILED=true" >> $GITHUB_OUTPUT
        else
          echo "âœ… **PASSED** - All performance metrics within acceptable ranges" >> performance-report.md
          echo "PERFORMANCE_GATE_FAILED=false" >> $GITHUB_OUTPUT
        fi

    - name: Upload performance artifacts
      uses: actions/upload-artifact@v3
      with:
        name: performance-test-results
        path: |
          performance-report.md
          locust_results_*.csv
          frontend/lighthouse-results.json
        retention-days: 7

    - name: Performance gate check
      id: performance-check
      run: |
        if [[ "${{ steps.performance-report.outputs.PERFORMANCE_GATE_FAILED }}" == "true" ]]; then
          echo "status=failed" >> $GITHUB_OUTPUT
          if [[ "${{ env.QUALITY_GATE_BLOCKING }}" == "true" ]]; then
            echo "ðŸ”’ Performance quality gate failed - blocking pipeline"
            exit 1
          else
            echo "âš ï¸  Performance quality gate failed - continuing (blocking disabled)"
          fi
        else
          echo "status=passed" >> $GITHUB_OUTPUT
          echo "âœ… Performance quality gate passed"
        fi

  integration-validation:
    runs-on: ubuntu-latest
    needs: [backend-test, frontend-test]
    outputs:
      integration-status: ${{ steps.integration-check.outputs.status }}
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'

    - name: Install dependencies
      run: |
        cd backend && pip install -r requirements.txt
        cd ../frontend && npm ci

    - name: Start services for integration testing
      run: |
        # Start backend service
        cd backend
        nohup python -m uvicorn app.main:app --host 0.0.0.0 --port 8000 &

        # Wait for backend to be ready
        timeout 60 bash -c 'until curl -f http://localhost:8000/health; do sleep 2; done'

        # Start frontend service
        cd ../frontend
        nohup npm start &
        timeout 60 bash -c 'until curl -f http://localhost:3000; do sleep 2; done'

    - name: API Contract Validation
      id: api-contract
      run: |
        echo "Running API contract validation..."

        # Check OpenAPI schema compliance
        curl -s http://localhost:8000/docs > api-docs.html || true

        # Test critical API endpoints
        endpoints=(
          "GET /health"
          "GET /api/workflows/"
          "GET /api/auth/me"
        )

        failed_endpoints=0
        for endpoint in "${endpoints[@]}"; do
          method=$(echo $endpoint | cut -d' ' -f1)
          path=$(echo $endpoint | cut -d' ' -f2)

          if [[ $method == "GET" ]]; then
            response=$(curl -s -o /dev/null -w "%{http_code}" http://localhost:8000$path)
            if [[ $response -lt 200 || $response -ge 400 ]]; then
              echo "âŒ Endpoint $method $path failed with status $response"
              ((failed_endpoints++))
            else
              echo "âœ… Endpoint $method $path responded with $response"
            fi
          fi
        done

        if [[ $failed_endpoints -gt 0 ]]; then
          echo "API_CONTRACT_FAILED=true" >> $GITHUB_OUTPUT
          echo "âŒ $failed_endpoints API endpoints failed"
        else
          echo "API_CONTRACT_FAILED=false" >> $GITHUB_OUTPUT
          echo "âœ… All API endpoints responding correctly"
        fi

    - name: Database Connection Validation
      id: db-validation
      run: |
        echo "Running database connection validation..."
        cd backend

        # Test database connection
        python -c "
        import os
        import sys
        sys.path.append('.')
        try:
            from app.database import engine
            from sqlalchemy import text

            with engine.connect() as conn:
                result = conn.execute(text('SELECT 1'))
                print('âœ… Database connection successful')
                sys.exit(0)
        except Exception as e:
            print(f'âŒ Database connection failed: {e}')
            sys.exit(1)
        " || { echo "DB_CONNECTION_FAILED=true" >> $GITHUB_OUTPUT; exit 0; }

        echo "DB_CONNECTION_FAILED=false" >> $GITHUB_OUTPUT

    - name: System Integration Tests
      id: system-integration
      run: |
        echo "Running system integration tests..."

        # Test RelayCore integration endpoints
        if curl -s http://localhost:8000/api/relaycore/status > /dev/null; then
          echo "âœ… RelayCore integration endpoint available"
          relaycore_status="success"
        else
          echo "âŒ RelayCore integration endpoint unavailable"
          relaycore_status="failed"
        fi

        # Test NeuroWeaver integration endpoints
        if curl -s http://localhost:8000/api/neuroweaver/status > /dev/null; then
          echo "âœ… NeuroWeaver integration endpoint available"
          neuroweaver_status="success"
        else
          echo "âŒ NeuroWeaver integration endpoint unavailable"
          neuroweaver_status="failed"
        fi

        if [[ $relaycore_status == "failed" || $neuroweaver_status == "failed" ]]; then
          echo "SYSTEM_INTEGRATION_FAILED=true" >> $GITHUB_OUTPUT
        else
          echo "SYSTEM_INTEGRATION_FAILED=false" >> $GITHUB_OUTPUT
        fi

    - name: Frontend-Backend Integration Test
      id: frontend-backend-integration
      run: |
        echo "Running frontend-backend integration tests..."

        # Test CORS configuration
        cors_response=$(curl -s -I -X OPTIONS http://localhost:8000/api/workflows/ | grep -i "access-control-allow-origin" | wc -l)
        if [[ $cors_response -gt 0 ]]; then
          echo "âœ… CORS configuration valid"
          cors_status="success"
        else
          echo "âŒ CORS configuration missing"
          cors_status="failed"
        fi

        # Test API response format
        api_response=$(curl -s http://localhost:8000/api/workflows/)
        if echo "$api_response" | jq . > /dev/null 2>&1; then
          echo "âœ… API returns valid JSON"
          json_status="success"
        else
          echo "âŒ API response is not valid JSON"
          json_status="failed"
        fi

        if [[ $cors_status == "failed" || $json_status == "failed" ]]; then
          echo "FRONTEND_BACKEND_INTEGRATION_FAILED=true" >> $GITHUB_OUTPUT
        else
          echo "FRONTEND_BACKEND_INTEGRATION_FAILED=false" >> $GITHUB_OUTPUT
        fi

    - name: Generate integration report
      id: integration-report
      run: |
        echo "## Integration Validation Report" > integration-report.md
        echo "- **Timestamp:** $(date)" >> integration-report.md
        echo "- **Commit:** ${{ github.sha }}" >> integration-report.md
        echo "- **Branch:** ${{ github.ref_name }}" >> integration-report.md
        echo "" >> integration-report.md

        echo "### Integration Test Results" >> integration-report.md
        echo "- **API Contract Validation:** ${{ steps.api-contract.outputs.API_CONTRACT_FAILED == 'false' && 'âœ… PASSED' || 'âŒ FAILED' }}" >> integration-report.md
        echo "- **Database Connection:** ${{ steps.db-validation.outputs.DB_CONNECTION_FAILED == 'false' && 'âœ… PASSED' || 'âŒ FAILED' }}" >> integration-report.md
        echo "- **System Integration:** ${{ steps.system-integration.outputs.SYSTEM_INTEGRATION_FAILED == 'false' && 'âœ… PASSED' || 'âŒ FAILED' }}" >> integration-report.md
        echo "- **Frontend-Backend Integration:** ${{ steps.frontend-backend-integration.outputs.FRONTEND_BACKEND_INTEGRATION_FAILED == 'false' && 'âœ… PASSED' || 'âŒ FAILED' }}" >> integration-report.md
        echo "" >> integration-report.md

        echo "### Integration Gate Status" >> integration-report.md
        if [[ "${{ steps.api-contract.outputs.API_CONTRACT_FAILED }}" == "true" || "${{ steps.db-validation.outputs.DB_CONNECTION_FAILED }}" == "true" || "${{ steps.system-integration.outputs.SYSTEM_INTEGRATION_FAILED }}" == "true" || "${{ steps.frontend-backend-integration.outputs.FRONTEND_BACKEND_INTEGRATION_FAILED }}" == "true" ]]; then
          echo "âŒ **FAILED** - Integration validation detected issues" >> integration-report.md
          echo "INTEGRATION_GATE_FAILED=true" >> $GITHUB_OUTPUT
        else
          echo "âœ… **PASSED** - All integration tests successful" >> integration-report.md
          echo "INTEGRATION_GATE_FAILED=false" >> $GITHUB_OUTPUT
        fi

    - name: Upload integration artifacts
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results
        path: integration-report.md
        retention-days: 7

    - name: Integration gate check
      id: integration-check
      run: |
        if [[ "${{ steps.integration-report.outputs.INTEGRATION_GATE_FAILED }}" == "true" ]]; then
          echo "status=failed" >> $GITHUB_OUTPUT
          if [[ "${{ env.QUALITY_GATE_BLOCKING }}" == "true" ]]; then
            echo "ðŸ”’ Integration quality gate failed - blocking pipeline"
            exit 1
          else
            echo "âš ï¸  Integration quality gate failed - continuing (blocking disabled)"
          fi
        else
          echo "status=passed" >> $GITHUB_OUTPUT
          echo "âœ… Integration quality gate passed"
        fi

  trivy-scan:
    runs-on: ubuntu-latest
    needs: [backend-test, frontend-test]
    steps:
    - uses: actions/checkout@v4

    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        format: 'sarif'
        output: 'trivy-results.sarif'
        severity: 'CRITICAL,HIGH'

    - name: Upload Trivy scan results
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'

  quality-gate-summary:
    needs: [backend-test, frontend-test, security-scan, performance-test, integration-validation, trivy-scan]
    runs-on: ubuntu-latest
    if: always()
    outputs:
      overall-status: ${{ steps.final-gate-check.outputs.status }}
    steps:
    - name: Generate comprehensive quality report
      id: quality-report
      run: |
        echo "## ðŸ” Comprehensive Quality Gate Report" > quality-gate-report.md
        echo "- **Timestamp:** $(date)" >> quality-gate-report.md
        echo "- **Commit:** ${{ github.sha }}" >> quality-gate-report.md
        echo "- **Branch:** ${{ github.ref_name }}" >> quality-gate-report.md
        echo "- **Pipeline:** ${{ github.workflow }}" >> quality-gate-report.md
        echo "" >> quality-gate-report.md

        echo "### ðŸ“Š Quality Gate Status Summary" >> quality-gate-report.md
        echo "" >> quality-gate-report.md

        # Backend Tests
        if [[ "${{ needs.backend-test.result }}" == "success" ]]; then
          echo "âœ… **Backend Tests:** PASSED" >> quality-gate-report.md
          backend_status="passed"
        else
          echo "âŒ **Backend Tests:** FAILED" >> quality-gate-report.md
          backend_status="failed"
        fi

        # Frontend Tests
        if [[ "${{ needs.frontend-test.result }}" == "success" ]]; then
          echo "âœ… **Frontend Tests:** PASSED" >> quality-gate-report.md
          frontend_status="passed"
        else
          echo "âŒ **Frontend Tests:** FAILED" >> quality-gate-report.md
          frontend_status="failed"
        fi

        # Security Scan
        if [[ "${{ needs.security-scan.outputs.security-status }}" == "passed" ]]; then
          echo "âœ… **Security Scan:** PASSED" >> quality-gate-report.md
          security_status="passed"
        else
          echo "âŒ **Security Scan:** FAILED" >> quality-gate-report.md
          security_status="failed"
        fi

        # Performance Test
        if [[ "${{ needs.performance-test.outputs.performance-status }}" == "passed" ]]; then
          echo "âœ… **Performance Test:** PASSED" >> quality-gate-report.md
          performance_status="passed"
        else
          echo "âŒ **Performance Test:** FAILED" >> quality-gate-report.md
          performance_status="failed"
        fi

        # Integration Validation
        if [[ "${{ needs.integration-validation.outputs.integration-status }}" == "passed" ]]; then
          echo "âœ… **Integration Validation:** PASSED" >> quality-gate-report.md
          integration_status="passed"
        else
          echo "âŒ **Integration Validation:** FAILED" >> quality-gate-report.md
          integration_status="failed"
        fi

        # Trivy Security Scan
        if [[ "${{ needs.trivy-scan.result }}" == "success" ]]; then
          echo "âœ… **Container Security (Trivy):** PASSED" >> quality-gate-report.md
          trivy_status="passed"
        else
          echo "âŒ **Container Security (Trivy):** FAILED" >> quality-gate-report.md
          trivy_status="failed"
        fi

        echo "" >> quality-gate-report.md
        echo "### ðŸ“ˆ Performance Metrics" >> quality-gate-report.md
        if [[ "${{ needs.performance-test.outputs.frontend-bundle-size }}" != "" ]]; then
          bundle_size_mb=$(echo "scale=2; ${{ needs.performance-test.outputs.frontend-bundle-size }} / 1024 / 1024" | bc -l 2>/dev/null || echo "N/A")
          echo "- **Frontend Bundle Size:** ${bundle_size_mb} MB" >> quality-gate-report.md
        fi
        if [[ "${{ needs.performance-test.outputs.api-response-time }}" != "" ]]; then
          echo "- **API Response Time:** ${{ needs.performance-test.outputs.api-response-time }} ms" >> quality-gate-report.md
        fi
        echo "" >> quality-gate-report.md

        echo "### ðŸŽ¯ Overall Quality Gate Status" >> quality-gate-report.md
        if [[ "$backend_status" == "failed" || "$frontend_status" == "failed" || "$security_status" == "failed" || "$performance_status" == "failed" || "$integration_status" == "failed" || "$trivy_status" == "failed" ]]; then
          echo "âŒ **OVERALL: FAILED** - Quality gates blocked the pipeline" >> quality-gate-report.md
          echo "QUALITY_GATES_FAILED=true" >> $GITHUB_OUTPUT
        else
          echo "âœ… **OVERALL: PASSED** - All quality gates passed successfully" >> quality-gate-report.md
          echo "QUALITY_GATES_FAILED=false" >> $GITHUB_OUTPUT
        fi

        echo "" >> quality-gate-report.md
        echo "### ðŸš¨ Failed Gates Details" >> quality-gate-report.md
        if [[ "$backend_status" == "failed" ]]; then
          echo "- **Backend Tests:** Unit tests or critical functionality failed" >> quality-gate-report.md
        fi
        if [[ "$frontend_status" == "failed" ]]; then
          echo "- **Frontend Tests:** Component tests or build process failed" >> quality-gate-report.md
        fi
        if [[ "$security_status" == "failed" ]]; then
          echo "- **Security Scan:** Vulnerabilities detected above threshold" >> quality-gate-report.md
        fi
        if [[ "$performance_status" == "failed" ]]; then
          echo "- **Performance Test:** Performance metrics below acceptable thresholds" >> quality-gate-report.md
        fi
        if [[ "$integration_status" == "failed" ]]; then
          echo "- **Integration Validation:** System integration issues detected" >> quality-gate-report.md
        fi
        if [[ "$trivy_status" == "failed" ]]; then
          echo "- **Container Security:** Container vulnerabilities detected" >> quality-gate-report.md
        fi

    - name: Upload quality gate report
      uses: actions/upload-artifact@v3
      with:
        name: quality-gate-report
        path: quality-gate-report.md
        retention-days: 7

    - name: Final quality gate enforcement
      id: final-gate-check
      run: |
        if [[ "${{ steps.quality-report.outputs.QUALITY_GATES_FAILED }}" == "true" ]]; then
          echo "status=failed" >> $GITHUB_OUTPUT
          echo "ðŸ”´ CRITICAL: Quality gates failed - pipeline blocked!"
          echo ""
          echo "ðŸ“‹ Failed Quality Gates:"
          if [[ "${{ needs.backend-test.result }}" != "success" ]]; then
            echo "  â€¢ Backend Tests"
          fi
          if [[ "${{ needs.frontend-test.result }}" != "success" ]]; then
            echo "  â€¢ Frontend Tests"
          fi
          if [[ "${{ needs.security-scan.outputs.security-status }}" != "passed" ]]; then
            echo "  â€¢ Security Scan"
          fi
          if [[ "${{ needs.performance-test.outputs.performance-status }}" != "passed" ]]; then
            echo "  â€¢ Performance Test"
          fi
          if [[ "${{ needs.integration-validation.outputs.integration-status }}" != "passed" ]]; then
            echo "  â€¢ Integration Validation"
          fi
          if [[ "${{ needs.trivy-scan.result }}" != "success" ]]; then
            echo "  â€¢ Container Security (Trivy)"
          fi
          echo ""
          echo "ðŸ“„ Please review the detailed reports in the workflow artifacts:"
          echo "  â€¢ Security Scan Results"
          echo "  â€¢ Performance Test Results"
          echo "  â€¢ Integration Test Results"
          echo "  â€¢ Quality Gate Report"
          echo ""
          echo "ðŸ”§ To disable blocking temporarily, set QUALITY_GATE_BLOCKING=false in environment variables"

          if [[ "${{ env.QUALITY_GATE_BLOCKING }}" == "true" ]]; then
            echo "ðŸ’¥ Pipeline will be blocked due to quality gate failures"
            exit 1
          else
            echo "âš ï¸  Quality gates failed but blocking is disabled - pipeline continuing"
          fi
        else
          echo "status=passed" >> $GITHUB_OUTPUT
          echo "ðŸŸ¢ SUCCESS: All quality gates passed!"
          echo ""
          echo "ðŸŽ‰ Pipeline ready for deployment"
          echo "ðŸ“Š All quality metrics met requirements:"
          echo "  â€¢ Security vulnerabilities: Within acceptable limits"
          echo "  â€¢ Performance metrics: Meeting SLAs"
          echo "  â€¢ Integration tests: All systems communicating"
          echo "  â€¢ Code quality: Passing all checks"
        fi
